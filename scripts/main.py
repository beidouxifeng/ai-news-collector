#!/usr/bin/env python3
"""
AIèµ„è®¯å“¨å…µ - ä¸»ç¨‹åº
å…¨è‡ªåŠ¨èµ„è®¯æŠ“å–ã€AIåˆ†æä¸æŠ¥å‘Šç”Ÿæˆç³»ç»Ÿ
"""

import os
import sys
import json
import logging
from datetime import datetime
from pathlib import Path

# Add parent directory to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from scripts.fetch_rss import RSSFetcher
from scripts.ai_processor import AIProcessor
from scripts.report_generator import ReportGenerator

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('news_sentinel.log')
    ]
)
logger = logging.getLogger(__name__)


def save_raw_data(articles: list, output_path: str = "data/articles.json"):
    """Save raw article data to JSON file"""
    output_file = Path(output_path)
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    # Convert datetime objects to string for JSON serialization
    serializable_articles = []
    for article in articles:
        article_copy = article.copy()
        if 'published' in article_copy:
            article_copy['published'] = article_copy['published'].isoformat()
        serializable_articles.append(article_copy)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(serializable_articles, f, ensure_ascii=False, indent=2)
    
    logger.info(f"âœ“ Raw data saved to {output_path}")


def main():
    """Main execution flow"""
    logger.info("="*60)
    logger.info("ğŸš€ AIèµ„è®¯å“¨å…µå¯åŠ¨")
    logger.info("="*60)
    
    try:
        # Step 1: Fetch RSS feeds
        logger.info("\nğŸ“¡ Step 1: æŠ“å–RSSæº...")
        fetcher = RSSFetcher()
        articles = fetcher.fetch_all()
        
        if not articles:
            logger.warning("æ²¡æœ‰æŠ“å–åˆ°ä»»ä½•æ–‡ç« ï¼Œç¨‹åºé€€å‡º")
            return
        
        # Deduplicate articles
        articles = fetcher.deduplicate(articles)
        logger.info(f"âœ“ è·å–åˆ° {len(articles)} ç¯‡å”¯ä¸€æ–‡ç« ")
        
        # Step 2: AI Analysis
        logger.info("\nğŸ¤– Step 2: AIæ™ºèƒ½åˆ†æ...")
        api_key = os.getenv('DEEPSEEK_API_KEY', '')
        
        if not api_key:
            logger.warning("âš ï¸  æœªé…ç½®DEEPSEEK_API_KEYï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼")
            logger.warning("   å¦‚éœ€AIåˆ†æï¼Œè¯·åœ¨GitHub Secretsä¸­é…ç½®DEEPSEEK_API_KEY")
        
        processor = AIProcessor(api_key=api_key)
        articles = processor.process_all(articles)
        
        # Sort by hot score
        articles = processor.sort_by_hot_score(articles)
        logger.info(f"âœ“ AIåˆ†æå®Œæˆ")
        
        # Step 3: Generate Report
        logger.info("\nğŸ“Š Step 3: ç”ŸæˆHTMLæŠ¥å‘Š...")
        generator = ReportGenerator()
        
        # Generate main report
        report_path = generator.generate_report(articles, "docs/index.html")
        
        # Create archive entry
        archive_path = generator.create_archive_entry(articles, "docs/archive")
        
        logger.info(f"âœ“ æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
        logger.info(f"  - æœ€æ–°æŠ¥å‘Š: {report_path}")
        logger.info(f"  - å†å²å­˜æ¡£: {archive_path}")
        
        # Step 4: Save raw data
        logger.info("\nğŸ’¾ Step 4: ä¿å­˜åŸå§‹æ•°æ®...")
        date_str = datetime.now().strftime('%Y-%m-%d')
        save_raw_data(articles, f"data/articles_{date_str}.json")
        
        # Print summary
        logger.info("\n" + "="*60)
        logger.info("ğŸ“Š è¿è¡Œæ‘˜è¦")
        logger.info("="*60)
        logger.info(f"æ€»æ–‡ç« æ•°: {len(articles)}")
        
        # Category breakdown
        categories = {}
        for article in articles:
            cat = article.get('category', 'Unknown')
            categories[cat] = categories.get(cat, 0) + 1
        
        logger.info("åˆ†ç±»åˆ†å¸ƒ:")
        for cat, count in sorted(categories.items(), key=lambda x: x[1], reverse=True):
            logger.info(f"  - {cat}: {count}")
        
        # Average hot score
        avg_score = sum(a.get('hot_score', 0) for a in articles) / len(articles) if articles else 0
        logger.info(f"å¹³å‡çƒ­åº¦åˆ†æ•°: {avg_score:.1f}")
        
        # Source breakdown (top 10)
        sources = {}
        for article in articles:
            source = article.get('source', 'Unknown')
            sources[source] = sources.get(source, 0) + 1
        
        logger.info("\nTop 10 èµ„è®¯æ¥æº:")
        for source, count in sorted(sources.items(), key=lambda x: x[1], reverse=True)[:10]:
            logger.info(f"  - {source}: {count}")
        
        logger.info("="*60)
        logger.info("âœ… æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")
        logger.info("="*60)
        
    except Exception as e:
        logger.error(f"âŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
